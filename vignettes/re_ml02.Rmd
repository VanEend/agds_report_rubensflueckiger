---
title: "re_ml02"
author: "Flueckiger_Rubens"
date: "2025-05-05"
output: 
  html_document:
    toc: true
---

# Report re_ml02

## Load packages and dataset

```{r, warning=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(readr)
library(tidyr)
library(caret)
library(recipes)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
davos <- read.table("../data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv", sep = ",", header = T)
laegern <- read.table("../data/FLX_CH-Lae.csv", sep = ",", header = T)
```

### Load function

The function optimal_k_eval takes a training set and a test set as input. With the training set the function creates a KNN-model, searches for a ideal k-value and models the GPP value with this ideal k value. Afterwards the $R^2$ value and the RMSE value are calculated with the test set and returned. Returned values: 
- First value: RMSE
- Second value: $R^2$

```{r, echo=FALSE}
source("../functions/optimal_k_eval.R")
```

## Preprocess Datasets

In this step every error value gets dropped and following variables are selected: - TIMESTAMP - GPP_NT_VUT_REF - TA_F - SW_IN_F - LW_IN_F - VPD_F - PA_F - P_F - WS_F

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#Davos
davos <- davos %>% # select only the variables we are interested in
  dplyr::select(TIMESTAMP,
                GPP_NT_VUT_REF,    # the target
                ends_with("_QC"),  # quality control info
                ends_with("_F"),   # includes all all meteorological covariates
                -contains("JSB")   # weird useless variable
                ) |>

  # convert to a nice date object
  dplyr::mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) |>

  # set all -9999 to NA
  dplyr::mutate(across(where(is.numeric), ~na_if(., -9999))) |> 

  
  # drop QC variables (no longer needed)
  dplyr::select(-ends_with("_QC"))

#Laegern
laegern <- laegern %>% # select only the variables we are interested in
  dplyr::select(TIMESTAMP,
                GPP_NT_VUT_REF,    # the target
                ends_with("_QC"),  # quality control info
                ends_with("_F"),   # includes all all meteorological covariates
                -contains("JSB")   # weird useless variable
                ) |>

  # convert to a nice date object
  dplyr::mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) |>

  # set all -9999 to NA
  dplyr::mutate(across(where(is.numeric), ~na_if(., -9999))) |> 

  # drop QC variables (no longer needed)
  dplyr::select(-ends_with("_QC"))
```

Then, a data set *total* containing data from Davos and Laegern is created

```{r, echo=FALSE}
total <- bind_rows(davos, laegern)
```

Afterwards all datasets are splited into training and test data

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Davos
set.seed(1982)  # for reproducibility
split <- rsample::initial_split(davos, prop = 0.8, strata = "VPD_F")
davos_train <- rsample::training(split)
davos_test <- rsample::testing(split)

#Laegern
set.seed(1982)  # for reproducibility
split <- rsample::initial_split(laegern, prop = 0.8, strata = "VPD_F")
laegern_train <- rsample::training(split)
laegern_test <- rsample::testing(split)

#Davos & Laegern
set.seed(1982)  # for reproducibility
split <- rsample::initial_split(total, prop = 0.8, strata = "VPD_F")
total_train <- rsample::training(split)
total_test <- rsample::testing(split)
```

## Modelling and evaluating

### Site Information

```{r, echo=FALSE, warning=FALSE, message=FALSE}
site_info <- load("../data/siteinfo_fluxnet2015.rda")
siteinfo_fluxnet2015 %>% 
  filter(sitename %in% c("CH-Dav", "CH-Lae"))
```

### Evaluation

```{r, echo=FALSE}
#With Davos
dav_v_dav <- optimal_k_eval(df_train = davos_train, df_test = davos_test)
dav_v_lae <- optimal_k_eval(df_train = davos_train, df_test = laegern_test)
dav_v_tot <- optimal_k_eval(df_train = davos_train, df_test = total_test)

#With Laegern
lae_v_dav <- optimal_k_eval(df_train = laegern_train, df_test = davos_test)
lae_v_lae <- optimal_k_eval(df_train = laegern_train, df_test = laegern_test)
lae_v_tot <- optimal_k_eval(df_train = laegern_train, df_test = total_test)

#With Davos & Laegern
tot_v_dav <- optimal_k_eval(df_train = total_train, df_test = davos_test)
tot_v_lae <- optimal_k_eval(df_train = total_train, df_test = laegern_test)
tot_v_tot <- optimal_k_eval(df_train = total_train, df_test = total_test)
```

| Model trained on data from Davos            |           $R^2$            |            RMSE            |
|:------------------------------------------|:-------------:|:-------------:|
| Evaluation against Davos test set           | `r round(dav_v_dav[2], 3)` | `r round(dav_v_dav[1], 3)` |
| Evaluation against Laegern test set         | `r round(dav_v_lae[2], 3)` | `r round(dav_v_lae[1], 3)` |
| Evaluation against Davos & Laegern test set | `r round(dav_v_tot[2], 3)` | `r round(dav_v_tot[1], 3)` |

| Model trained on data from Laegern          |           $R^2$            |            RMSE            |
|:------------------------------------------|:-------------:|:-------------:|
| Evaluation against Davos test set           | `r round(lae_v_dav[2], 3)` | `r round(lae_v_dav[1], 3)` |
| Evaluation against Laegern test set         | `r round(lae_v_lae[2], 3)` | `r round(lae_v_lae[1], 3)` |
| Evaluation against Davos & Laegern test set | `r round(lae_v_tot[2], 3)` | `r round(lae_v_tot[1], 3)` |

| Model trained on data from Davos & Laegern  |           $R^2$            |            RMSE            |
|:------------------------------------------|:-------------:|:-------------:|
| Evaluation against Davos test set           | `r round(tot_v_dav[2], 3)` | `r round(tot_v_dav[1], 3)` |
| Evaluation against Laegern test set         | `r round(tot_v_lae[2], 3)` | `r round(tot_v_lae[1], 3)` |
| Evaluation against Davos & Laegern test set | `r round(tot_v_tot[2], 3)` | `r round(tot_v_tot[1], 3)` |

## Interpretation

**What are the patterns that you see in the tables?**

In general one can see that a model trained on one site has a better performance if tested on the same site. However, one can see that the RMSE value for both Laegern and Davos & Laegern the lowest value cannot be seen at the test set from the same site. Nontheless, the difference between its value and the lowest value is very small such that the general observation still holds.

**How well do models trained on one site perform on another site? Why is this the case?**

Quite poor. Especially for a set trained in Davos and tested in Laegern. The difference for a set trained in Laegern and tested in Davos is less dramatic.

In general the differences occur due to height differences (Davos is about 1'000m higher than Laegern) and climatic differences. As Davos is according to the table above in a Polar-Tundra zone (ET) and Laegern is in a Temperate-No_dry_season-Warm_summer zone (Cfb) difference have to occur. In addition, the duration of the sampling for Davos is longer than the sampling duration in Laegern. There might occur some kind of an anomaly during the period when Davos was sampling and Laegern was not, which can have an affect on the performance of the models.

I think that the difference, especially in the contrast between Davos tested in Laegern and Laegern tested in Davos, has to do with the foliage type observed at both sites. As Davos has predominantly evergreen needle leaf trees, the model is trained with a specialization on needle leaf trees only. Therefore, the model performs poorer for a site with mixed forest conditions. A site with mixed forest conditions however contains to some extent needle leaf trees and therefore can still perform quite alright in a forest with only one type of tree.

**How does the model trained on both sites perform on the three test sets? Why is this the case?**

The model trained on both sites does more or less identically well on all test sites.

As already mentioned in the third paragraph on the second question, a model trained on a diverse site performs well both on a homogeneous forest site (like in Davos) and a heterogeneous forest site (like in Laegern). If a data set contains data from both sites this effect is further enhanced.

**When training and testing on both sites, is this a true ‘out-of-sample’ setup? What would you expect if your model was tested against data from a site in Spain?**

As I have splited training and test data, no. There might be training information from the data set containing both sites on a testing set inside one of the testing sets of Davos or Laegern. In hindsight it might be a better idea to split training and test set for Davos and Laegern first and combining training and test set from Davos and Laegern afterwards to form the training and test set. Then however the size of the training and testing set does not reach the required size given in the task.

I assume the model will perform poor for a site in Spain. Both Davos and Laegern show a more similar climatic condition (Temperate vs. Tundra) whereas a site in Spain would show a more dry condition. Therefore, the implementation of the model on a site in Spain would fail as the model was not trained with foliage growing especially these region.
